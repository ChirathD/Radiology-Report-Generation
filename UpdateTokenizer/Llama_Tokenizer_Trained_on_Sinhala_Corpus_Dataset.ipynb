{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C029srGDObcz"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install accelerate peft bitsandbytes transformers trl"
      ],
      "metadata": {
        "id": "TvYywq7MOlu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "V3zFIqRHO2a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')"
      ],
      "metadata": {
        "id": "EGJj9A7AO_n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sinhala_corpus = load_dataset('LexiconShiftInnovations/SinhalaCorpusLarge')\n",
        "dataset_sinhala_wiki = load_dataset('LexiconShiftInnovations/SinhalaWikipediaArticles')\n",
        "dataset_sinhala_dental_qna = load_dataset('LexiconShiftInnovations/SinhalaDentalQnA')"
      ],
      "metadata": {
        "id": "8c2f49RfU1th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sinhala_dental_qna_text = dataset_sinhala_dental_qna['train']['text']\n",
        "dataset_sinhala_dental_qna_dataset = {\"text\": dataset_sinhala_dental_qna_text}\n",
        "dataset_sinhala_dental_qna_dataset = Dataset.from_dict(dataset_sinhala_dental_qna_dataset)"
      ],
      "metadata": {
        "id": "7fBdeRJeWwCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_to_concatenate = [dataset_sinhala_corpus['train'], dataset_sinhala_wiki['train'], dataset_sinhala_dental_qna_dataset]"
      ],
      "metadata": {
        "id": "IeBdw6VRYUak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sinhala_Corpus_Train = concatenate_datasets(datasets_to_concatenate)"
      ],
      "metadata": {
        "id": "zGe8w57PXOYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables_to_delete = ['dataset_sinhala_corpus', 'dataset_sinhala_wiki', 'dataset_sinhala_dental_qna', 'dataset_sinhala_dental_qna_dataset']\n",
        "%reset_selective -f {variable for variable in variables_to_delete}\n"
      ],
      "metadata": {
        "id": "BISwpmd-dNQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sinhala_Corpus_Train"
      ],
      "metadata": {
        "id": "qC3Lh7FjXxE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    return (\n",
        "        Sinhala_Corpus_Train['text'][i : i + 1000]\n",
        "        for i in range(0, len(Sinhala_Corpus_Train[\"text\"]), 1000)\n",
        "    )\n",
        "\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "CkYyJQ6-REP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_llama_tokenizer = old_llama_tokenizer.train_new_from_iterator(training_corpus, 20000)"
      ],
      "metadata": {
        "id": "l9k_4xKSR6G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_llama_tokenizer.push_to_hub('LexiconShiftInnovations/Llama2SinhalaTokenizer')"
      ],
      "metadata": {
        "id": "B2ahS9hYf5gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sinhala_text = \"ඉස්සරහ දත් දෙක මැද හිඩස පිරෙව්වට පස්සේ mouth wash එකක් භාවිතා කිරීම නුසුදුසු ද?\"\n",
        "\n",
        "\n",
        "llama_2_existing_tokenizer_output = old_llama_tokenizer.tokenize(sinhala_text)\n",
        "llama_2_sinhala_tokenizer_output = new_llama_tokenizer.tokenize(sinhala_text)\n",
        "\n",
        "print(\"Output from the existing Llama-2 Tokenizer\")\n",
        "print(f\"Token count : {len(llama_2_existing_tokenizer_output)}\")\n",
        "\n",
        "print(llama_2_existing_tokenizer_output)\n",
        "print(\"\\nOutput from the Llama-2 Tokenizer trained on Sinhala Corpus\")\n",
        "print(f\"Token count : {len(llama_2_sinhala_tokenizer_output)}\")\n",
        "print(llama_2_sinhala_tokenizer_output)"
      ],
      "metadata": {
        "id": "wK_MNcymSWL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84orvpcRSmUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_vocab = old_llama_tokenizer.vocab\n",
        "new_vocab = new_llama_tokenizer.vocab"
      ],
      "metadata": {
        "id": "ERGi761NDT7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(old_vocab)"
      ],
      "metadata": {
        "id": "TZhTEMMpFkks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_vocab)"
      ],
      "metadata": {
        "id": "OCz6qfs2Fn22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_old_vocab = set(old_vocab)\n",
        "set_new_vocab = set(new_vocab)\n",
        "\n",
        "intersection_set = set_old_vocab.intersection(set_new_vocab)"
      ],
      "metadata": {
        "id": "ahrRLLypFwSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(intersection_set)"
      ],
      "metadata": {
        "id": "cGa2dUhwGov5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_to_add = set_new_vocab - intersection_set"
      ],
      "metadata": {
        "id": "spD6s_4OGsda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_llama_tokenizer.add_tokens(list(tokens_to_add))"
      ],
      "metadata": {
        "id": "9iz7MSZXG_f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(old_llama_tokenizer.vocab)"
      ],
      "metadata": {
        "id": "HcuNP1SWHJlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_llama_tokenizer = old_llama_tokenizer.tokenize(sinhala_text)"
      ],
      "metadata": {
        "id": "pGb5YLrlHPPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Token count : {len(updated_llama_tokenizer)}\")\n",
        "print(updated_llama_tokenizer)"
      ],
      "metadata": {
        "id": "5oEu4owxTian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rarCeN2nTncx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}